
[]{#ch02}Chapter 2. Applying Machine Learning to Structured Data {#chapter-2.-applying-machine-learning-to-structured-data .title}
----------------------------------------------------------------

</div>

</div>
:::

Structured data is a term used for any[]{#id77 .indexterm} data that
resides in a fixed field within a record or file, two such examples
being relational databases and spreadsheets. Usually, structured data is
presented in a table in which each column presents a type of value, and
each row represents a new entry. Its structured format means that this
type of data lends itself to classical statistical analysis, which is
also why most data science and analysis work is done on structured data.

In day-to-day life, structured data is also the most common type of data
available to businesses, and most machine learning problems that need to
be solved in finance deal with structured data in one way or another.
The fundamentals of any modern company\'s day-to-day running is built
around structured data, including, transactions, order books, option
prices, and suppliers, which are all examples of information usually
collected in spreadsheets or databases.

This chapter will walk you through a structured data problem involving
credit card fraud, where we will use feature engineering to identify the
fraudulent transaction from a dataset successfully. We\'ll also
introduce the basics of an [**end-to-end**]{.strong}
([**E2E**]{.strong}) approach so that we can solve common financial
problems.

Fraud is an unfortunate reality that all financial institutions have to
deal with. It\'s a constant race between companies trying to protect
their systems and fraudsters who are trying to defeat the protection in
place. For a long time, fraud detection has relied on simple heuristics.
For example, a large transaction made while you\'re in a country you
usually don\'t live in will likely result in that transaction being
flagged.

Yet, as fraudsters continue to understand and circumvent the rules,
credit card providers are deploying increasingly sophisticated machine
learning systems to counter this.

In this chapter, we\'ll look at how a real bank might tackle the problem
of fraud. It\'s a real-world exploration of how a team of data
scientists starts with a heuristic baseline, then develops an
understanding of its features, and from that, builds increasingly
sophisticated machine learning models that can detect fraud. While the
data we will use is synthetic, the process of development and tools that
we\'ll use to tackle fraud are similar to the tools and processes that
are used every day by international retail banks.

So where do you start? To put it in the words of one anonymous fraud
detection expert that I spoke to, [*\"I keep thinking about how I would
steal from my employer, and then I create some features that would catch
my heist. To catch a fraudster, think like a fraudster.\"*]{.emphasis}
Yet, even the most ingenious feature []{#id78 .indexterm}engineers are
not able to pick up on all the subtle and sometimes counterintuitive
signs of fraud, which is why the industry is slowly shifting toward
entirely E2E-trained systems. These systems, in addition to machine
learning, are both focuses of this chapter where we will explore several
commonly used approaches to flag fraud.

This chapter will act as an important baseline to [Chapter
6](https://subscription.packtpub.com/book/data/9781789136364/6){.link},
[*Using Generative Models*]{.emphasis}, where we will again be
revisiting the credit card fraud problem for a full E2E model using
auto-encoders.



[]{#ch02lvl1sec27}The data {#the-data .title style="clear: both"}
--------------------------

</div>

</div>

------------------------------------------------------------------------
:::

The dataset we[]{#id79 .indexterm} will work with is a synthetic dataset
of transactions generated by a payment simulator. The goal of this case
study and the focus of this chapter is to find fraudulent transactions
within a dataset, a classic machine learning problem many financial
institutions deal with.

::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### []{#note05}Note {#note .title}

[**Note**]{.strong}: Before we go further, a digital copy of the code,
as well as an interactive notebook for this chapter are accessible
online, via the following two links:

An interactive notebook containing the code for this chapter can be
found under <https://www.kaggle.com/jannesklaas/structured-data-code>

The code can also be found on GitHub, in this book\'s repository:
<https://github.com/PacktPublishing/Machine-Learning-for-Finance>
:::

The dataset we\'re using stems from the paper [*PaySim: A financial
mobile money simulator for fraud detection*]{.emphasis}, by E. A.
Lopez-Rojas, A. Elmir, and S. Axelsson. The dataset can be found
on[]{#id80 .indexterm} Kaggle under this URL:
<https://www.kaggle.com/ntnu-testimon/paysim1>.

Before we break it down on the next page, let\'s take a minute to look
at the dataset that we\'ll be using in this chapter. Remember, you can
download the data with the preceding link.

::: {.informaltable}
  step   type        amount   nameOrig      oldBalance Orig   newBalance Orig   nameDest      oldBalance Dest   newBalance Dest   isFraud   isFlagged Fraud
  ------ ----------- -------- ------------- ----------------- ----------------- ------------- ----------------- ----------------- --------- -----------------
  1      PAYMENT     64       C1231006815   0                 36                M1979787155   0                 0                 0         0
  1      PAYMENT     28       C1666544295   0                 72                M2044282225   0                 0                 0         0
  1      TRANSFER    0        C1305486145   0                 0                 C553264065    0                 0                 1         0
  1      CASH\_OUT   0        C840083671    0                 0                 C38997010     0                 0                 1         0
  1      PAYMENT     14       C2048537720   0                 86                M1230701703   0                 0                 0         0
  1      PAYMENT     71       C90045638     0                 29                M573487274    0                 0                 0         0
  1      PAYMENT     77       C154988899    0                 23                M408069119    0                 0                 0         0
  1      PAYMENT     64       C1912850431   23                59                M633326333    0                 0                 0         0
  1      PAYMENT     36       C1265012928   0                 0                 M1176932104   0                 0                 0         0
  1      DEBIT       77       C712410124    0                 23                C195600860    0                 79                0         0
:::

As seen in the first[]{#id81 .indexterm} row, the dataset has 11
columns. Let\'s explain what each one represents before we move on:

::: {.itemizedlist}
-   [**step**]{.strong}: Maps time, with each step corresponding to one
    hour.

-   [**type**]{.strong}: The type of the transaction, which can be
    CASH\_IN, CASH\_OUT, DEBIT, PAYMENT, or TRANSFER.

-   [**amount**]{.strong}: The amount of the transaction.

-   [**nameOrig**]{.strong}: The origin account that started the
    transaction. C relates to customer accounts, while M is the account
    of merchants.

-   [**oldbalanceOrig**]{.strong}: The old balance of the origin
    account.

-   [**newbalanceOrig**]{.strong}: The new balance of the origin account
    after the transaction amount has been added.

-   [**nameDest**]{.strong}: The destination account.

-   [**oldbalanceDest**]{.strong}: The old balance of the destination
    account. This information is not available for merchant accounts
    whose names start with M.

-   [**newbalanceDest**]{.strong}: The new balance of the destination
    account. This information is not available for merchant accounts.

-   [**isFraud**]{.strong}: Whether the transaction was fraudulent.

-   [**isFlaggedFraud**]{.strong}: Whether the old system has flagged
    the transaction as fraud.
:::

In the preceding table, we []{#id82 .indexterm}can see 10 rows of data.
It\'s worth noting that there are about 6.3 million transactions in our
total dataset, so what we\'ve seen is a small fraction of the total
amount. As the fraud we\'re looking at only occurs in transactions
marked as either TRANSFER or CASH\_OUT, all other transactions can be
dropped, leaving us with around 2.8 million examples to work with.



[]{#ch02lvl1sec28}Heuristic, feature-based, and E2E models {#heuristic-feature-based-and-e2e-models .title style="clear: both"}
----------------------------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

Before we dive into[]{#id83 .indexterm} developing models to detect
fraud, let\'s take a second to pause and ponder []{#id84 .indexterm}over
the different kinds of models we could build.

::: {.itemizedlist}
-   A heuristic-based []{#id85 .indexterm}model is a simple \"rule of
    thumb\" developed purely by humans. Usually, the heuristic model
    stems from having an expert knowledge of the problem.

-   A feature-based model relies heavily on humans modifying the data to
    create new and meaningful features, which are then fed into a
    (simple) machine learning algorithm. This approach mixes expert
    knowledge with learning from data.

-   An E2E model learns purely from raw data. No human expertise is
    used, and the model learns everything directly from observations.
:::

In our case, a heuristic-based model could be created to mark all
transactions with the TRANSFER transaction type and an amount over
\$200,000 as fraudulent. Heuristic-based models have the advantage that
they are both fast to develop and easy to implement; however, this comes
with a pay-off, their performance is often poor, and fraudsters can
easily play the system. Let\'s imagine that we went with the preceding
heuristic-based model, fraudsters transferring only \$199,999, under the
fraudulent limit, would evade detection.

An important heuristic in the field of trading is the momentum strategy.
Momentum strategies involve betting that a stock that\'s on the rise
will continue to rise, with people then buying that stock. While this
strategy sounds too simple to be any good, it is in fact, a reasonably
successful strategy that many high-frequency trading and quantitative
outlets are using today.

To create features, experts craft indicators that can distinguish
fraudulent transactions from those that are genuine. This is often done
using statistical data analysis, and when compared to the
heuristic-based model that we proposed early on, it will take longer,
but with the benefit of better results.

Feature engineering-based models[]{#id86 .indexterm} are a midway
between data and humans []{#id87 .indexterm}shaping rules, where human
knowledge[]{#id88 .indexterm} and creativity are exploited to craft good
features, and data and machine learning are used to create a model from
those features.

E2E models learn purely from collected data without using expert
knowledge. As discussed before, this often yields much better results,
but at the cost of taking a lot of time to complete. This method also
has some additional elements worth considering. For instance, collecting
the large amount of data that will be needed is an expensive task, as
humans have to label millions of records.

Though for many people in the industry right now, they take the view
that shipping a poor model is often better than not shipping anything at
all. After all, having some protection against fraud is better than
simply having none.

Using a heuristic approach that lets through half of all fraudulent
transactions is better than having no fraud detection at all. The graph
shows us the performance of the three models we introduced earlier on,
against the time taken to implement them.

::: {.mediaobject}
![](3_files/B10354_02_01.jpg)

::: {.caption}
The methods used and the performance of the system during development
:::
:::

The best method is to use a combination of all three. If we deploy a
heuristic model that meets the basic requirements of the task that it
set out to achieve, then it can be shipped. By employing this []{#id89
.indexterm}method, the heuristic then becomes the baseline that any
other approach has to beat. Once your heuristic model is deployed, then
all your efforts should then be directed toward building a feature-based
model, which as soon as it beats the initially deployed heuristic model,
can then be deployed while you continue to refine the model.

As we\'ve []{#id90 .indexterm}discussed before, feature-based models
often deliver pretty decent performance on structured data tasks; this
gives companies the time to undertake the[]{#id91 .indexterm} lengthy
and expensive task of building an E2E model, which can be shipped once
it beats the feature-based model. Now that we understand the type
of models we\'re going to build, let\'s look at the software we need to
build them.



[]{#ch02lvl1sec29}The machine learning software stack {#the-machine-learning-software-stack .title style="clear: both"}
-----------------------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

In this chapter, we will be using a range of different libraries that
are commonly used in machine learning. Let\'s take a minute[]{#id92
.indexterm} to look at our stack, which consists of the following
software:

::: {.itemizedlist}
-   [**Keras**]{.strong}: A neural network library that can act as a
    simplified interface to TensorFlow.

-   [**NumPy**]{.strong}: Adds support[]{#id93 .indexterm} for large,
    multidimensional arrays as well as an extensive collection of
    mathematical functions.

-   [**Pandas**]{.strong}: A library []{#id94 .indexterm}for data
    manipulation and analysis. It\'s similar to Microsoft\'s Excel but
    in Python, as it offers data structures to handle tables and the
    tools to manipulate them.

-   [**Scikit-learn**]{.strong}: A[]{#id95 .indexterm} machine learning
    library offering a wide range of algorithms and utilities.

-   [**TensorFlow**]{.strong}: A dataflow programming library that
    facilitates working with neural networks.

-   [**Matplotlib**]{.strong}: A[]{#id96 .indexterm} plotting library.

-   [**Jupyter**]{.strong}: A[]{#id97 .indexterm} development
    environment. All of the code examples in this book are available in
    Jupyter Notebooks.
:::

The majority of this book is dedicated to working with the Keras
library, while this chapter makes extensive[]{#id98 .indexterm} use of
the other libraries mentioned. The goal here is less about teaching you
all the tips and tricks of all the different libraries, but more about
showing you how they are integrated into the process of creating a
predictive model.

::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### []{#note06}Note {#note .title}

[**Note**]{.strong}: All of the libraries needed for this chapter are
installed on Kaggle kernels by default. If you are running this code
locally, please refer to the setup instructions in [Chapter
1](https://subscription.packtpub.com/book/data/9781789136364/1){.link},
[*Neural Networks and Gradient-Based Optimization*]{.emphasis}, and
install all of the libraries needed.



[]{#ch02lvl1sec30}The heuristic approach {#the-heuristic-approach .title style="clear: both"}
----------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

Earlier in this chapter, we[]{#id99 .indexterm} introduced the three
models that we will be using to detect fraud, now it\'s time to explore
each of them in more detail. We\'re going to start with the heuristic
approach.

Let\'s start by defining a simple heuristic model and measuring how well
it does at measuring fraud rates.

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec17}Making predictions using the heuristic model {#making-predictions-using-the-heuristic-model .title}

</div>

</div>
:::

We will be making our[]{#id100 .indexterm} predictions using the
heuristic approach over the entire training data set in order to get an
idea of how well this heuristic model does at predicting fraudulent
transactions.

The following code will create a new column,
`Fraud_Heuristic`{.literal}, and in turn assigns a value of
`1`{.literal} in rows where the type is `TRANSFER`{.literal}, and the
amount is more than \$200,000:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
df['Fraud_Heuristic '] = np.where(((df['type'] == 'TRANSFER') &(df['amount'] > 200000)),1,0)
```
:::

With just two lines of code, it\'s easy to see how such a simple metric
can be easy to write, and quick to deploy.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec18}The F1 score {#the-f1-score .title}

</div>

</div>
:::

One important thing[]{#id101 .indexterm} we must consider is the need
for a common metric on which we can evaluate all of our models on. In
[Chapter
1](https://subscription.packtpub.com/book/data/9781789136364/1){.link},
[*Neural Networks and Gradient-Based Optimization*]{.emphasis}, we used
accuracy as our emulation tool. However, as we\'ve seen, there are far
fewer fraudulent transactions than there are genuine ones. Therefore a
model that classifies all the transactions as genuine can have a very
high level of accuracy.

One such metric that[]{#id102 .indexterm} is designed to deal with such
a skewed distribution is the F1 score, which considers true and false
positives and negatives, as you can see in this chart:

::: {.informaltable}
                    Predicted Negative                                  Predicted Positive
  ----------------- --------------------------------------------------- ---------------------------------------------------
  Actual Negative   [**True Negative**]{.strong} ([**TN**]{.strong})    [**False Positive**]{.strong} ([**FP**]{.strong})
  Actual Positive   [**False Negative**]{.strong} ([**FN**]{.strong})   [**True Positive**]{.strong} ([**TP**]{.strong})
:::

We can first compute the precision of our model, which specifies the
share of predicted positives that were positives, using the following
formula:

::: {.mediaobject}
![](5_files/B10354_02_001.jpg)
:::

Recall measures the share of predicted positives over the actual number
of positives, as seen in this formula:

::: {.mediaobject}
![](5_files/B10354_02_002.jpg)
:::

The F1 score is then calculated from the harmonic mean, an average, of
the two measures, which can be seen in the following formula:

::: {.mediaobject}
![](5_files/B10354_02_003.jpg)
:::

To compute this metric in Python, we can use the `metrics`{.literal}
module of scikit-learn, or sklearn for short:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from sklearn.metrics import f1_score
```
:::

Given the predictions we\'ve made, we can now easily compute the F1
score using the following command:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
f1_score(y_pred=df['Fraud_Heuristic '],y_true=df['isFraud'])
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.013131315551742895
```
:::

You\'ll see that the preceding command outputs a number--starting
0.013131315...-What this number means exactly is that our heuristic
model is not doing too well, as the best possible F1 score is 1, and the
worst is 0. In our case, this number[]{#id103 .indexterm} represents the
harmonic mean of the share of correctly caught frauds over everything
labeled as fraud and the share of correctly caught frauds over all
frauds.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec19}Evaluating with a confusion matrix {#evaluating-with-a-confusion-matrix .title}

</div>

</div>
:::

A more qualitative[]{#id104 .indexterm} and interpretable way of
evaluating a model is with a confusion matrix. As the name suggests, the
matrix shows how our[]{#id105 .indexterm} classifier confuses classes.

Firstly, let\'s study the code appendix for the
`plot_confusion_matrix`{.literal} function:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from sklearn.metrics import confusion_matrix cm = confusion_matrix(
    y_pred=df['Fraud_Heuristic '],y_true=df['isFraud']) 
plot_confusion_matrix(cm,['Genuine','Fraud'])
```
:::

Which, when we run, produces the following graphic:

::: {.mediaobject}
![](5_files/B10354_02_02.jpg)

::: {.caption}
A confusion matrix for a heuristic model
:::
:::

So, just how accurate was that model? As you can see in our confusion
matrix, from our dataset of 2,770,409 examples, 2,355,826 were correctly
classified as genuine, while 406,370 were falsely classified as fraud.
In fact, only 2,740 examples were correctly classified as fraud.

When our heuristic []{#id106 .indexterm}model classified a transaction
as fraudulent, it was genuine in 99.3% of those cases. Only 34.2% of the
total frauds got []{#id107 .indexterm}caught. All this information is
incorporated into the F1 score we formulated. However, as we saw, it is
easier to read this from the generated confusion matrix graphic. The
reason we used both the heuristic model and the F1 score is that it is
good practice to have a single number that tells us which model is
better, and also a more graphical insight into how that model is better.

To put it frankly, our heuristic model has performed quite poorly,
detecting only 34.2% of fraud, which is not good enough. So, using the
other two methods in the following sections, we\'re going to see whether
we can do better.



[]{#ch02lvl1sec31}The feature engineering approach {#the-feature-engineering-approach .title style="clear: both"}
--------------------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

The objective of feature engineering is to exploit the qualitative
insight of humans in order to create better machine learning models. A
human engineer usually uses three types of insight:
[*intuition*]{.emphasis}, [*expert domain knowledge*]{.emphasis}, and
[*statistical analysis*]{.emphasis}. Quite often, it\'s possible to come
up with features for a problem just from intuition.

As an example, in[]{#id108 .indexterm} our fraud case, it seems
intuitive that fraudsters will create new accounts for their fraudulent
schemes and won\'t be using the same bank account that they pay for
their groceries with.

Domain experts are able to use their extensive knowledge of a problem in
order to come up with other such examples of intuition. They\'ll know
more about how fraudsters behave and can craft features that indicate
such behavior. All of these intuitions are then usually confirmed by
statistical analysis, something that can even be used to open the
possibilities of discovering new features.

Statistical analysis can sometimes turn up quirks that can be turned
into predictive features. However, with this method, engineers[]{#id109
.indexterm} must beware of the [**data trap**]{.strong}. Predictive
features found in the data might only exist in that data because any
dataset will spit out a predictive feature if it\'s wrangled with for
long enough.

A data trap refers to engineers digging within the data for features
forever, and never questioning whether those features they are searching
for are relevant.

Data scientists stuck in to the data trap keep euphorically finding
features, only to realize later that their model, with all those
features, does not work well. Finding strong predictive features in the
training set is like a drug for data science teams. Yes, there\'s an
immediate reward, a quick win that feels like a validation of one\'s
skills. However, as with many drugs, the data trap can lead to an
after-effect in which teams find that weeks\' or months\' worth of work
in finding those features was actually, useless.

Take a minute to ask yourself, are you in that position? If you ever
find yourself applying analysis after analysis, transforming data in
every possible way, chasing correlation values, you might very well be
stuck in a data trap.

To avoid the data trap, it is important to establish a [**qualitative
rationale**]{.strong} as to why this statistical predictive feature
exists and []{#id110 .indexterm}should exist outside of the dataset as
well. By establishing this rationale, you will keep both yourself and
your team alert to avoiding crafting features that represent noise. The
data trap is the human form of overfitting and finding patterns in
noise, which is a problem for models as well.

Humans can use[]{#id111 .indexterm} their qualitative reasoning skills
to avoid fitting noise, which is a big advantage humans have over
machines. If you\'re a data scientist, you should use this skill to
create more generalizable models.

The goal of this section was not to showcase all the features that
feature engineering could perform on this dataset, but just to highlight
the three approaches and how they can be turned into features.

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec20}A feature from intuition -- fraudsters don\'t sleep {#a-feature-from-intuition-fraudsters-dont-sleep .title}

</div>

</div>
:::

Without knowing much about fraud, we can intuitively describe fraudsters
as shady people that operate in the dark. In most cases, genuine
transactions happen during the day, as people sleep at night.

The time steps in[]{#id112 .indexterm} our dataset represent one hour.
Therefore, we can generate the time of the day by simply taking the
remainder of a division by 24, as seen in this code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
df['hour'] = df['step'] % 24
```
:::

From there, we can then count the number of fraudulent and genuine
transactions at different times. To calculate this, we must run the
following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
frauds = []
genuine = []
for i in range(24):
    f = len(df[(df['hour'] == i) & (df['isFraud'] == 1)])
    g = len(df[(df['hour'] == i) & (df['isFraud'] == 0)])
    frauds.append(f)
    genuine.append(g)
```
:::

Then finally, we can plot the share of genuine and fraudulent
transactions over the course of the day into a chart. To do this, we
must run the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
fig, ax = plt.subplots(figsize=(10,6)) 
ax.plot(genuine/np.sum(genuine), label='Genuine') 
ax.plot(frauds/np.sum(frauds),dashes=[5, 2], label='Fraud') 
plt.xticks(np.arange(24))
legend = ax.legend(loc='upper center', shadow=True)
```
:::

::: {.mediaobject}
![](6_files/B10354_02_03.jpg)

::: {.caption}
The share of fraudulent and genuine transactions conducted throughout
each hour of the day
:::
:::

As we can see in the preceding chart, there are much fewer genuine
transactions at night, while fraudulent[]{#id113 .indexterm} behavior
continues over the day. To be sure that night is a time when we can hope
to catch fraud, we can also plot the number of fraudulent transactions
as a share of all transactions. To do this, we must run the following
command:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
fig, ax = plt.subplots(figsize=(10,6))
ax.plot(np.divide(frauds,np.add(genuine,frauds)), label='Share of fraud')
plt.xticks(np.arange(24))
legend = ax.legend(loc='upper center', shadow=True)
```
:::

::: {.mediaobject}
![](6_files/B10354_02_04.jpg)

::: {.caption}
The share of transactions that are fraudulent per hour of the day
:::
:::

Once we run that code, we can see that at around 5 AM, over 60% of all
transactions[]{#id114 .indexterm} seem to be fraudulent, which appears
to make this a great time of the day to catch fraud.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec21}Expert insight -- transfer, then cash out {#expert-insight-transfer-then-cash-out .title}

</div>

</div>
:::

The description of the dataset came with another description that
explained the expected behavior of[]{#id115 .indexterm} fraudsters.
First, they transfer money to a bank account they control. Then, they
cash out that money from an ATM.

We can check whether there[]{#id116 .indexterm} are fraudulent transfer
destination accounts that are the origin of the fraudulent cash outs by
running the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
dfFraudTransfer = df[(df.isFraud == 1) & (df.type == 'TRANSFER')]
dfFraudCashOut = df[(df.isFraud == 1) & (df.type == 'CASH_OUT')]
dfFraudTransfer.nameDest.isin(dfFraudCashOut.nameOrig).any()
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: False
```
:::

According to the output, there seems to be no fraudulent transfers that
are the origin of fraudulent cash outs. The behavior expected by the
experts is not visible in our data. This could mean two things: firstly,
it could mean that the fraudsters behave differently now, or secondly
that our data does not capture their behavior. Either way, we cannot use
this insight for predictive modeling here.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec22}Statistical quirks -- errors in balances {#statistical-quirks-errors-in-balances .title}

</div>

</div>
:::

A closer examination[]{#id117 .indexterm} of the data shows that there
are some transactions where the old and new balances of the destination
account is zero, although the transaction amount is not zero. This is
odd, or more so a quirk, and so we want to investigate whether this type
of oddity yields predictive power.

To begin with, we can calculate the share of fraudulent transactions
with this property by running the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
dfOdd = df[(df.oldBalanceDest == 0) &
           (df.newBalanceDest == 0) & 
           (df.amount)]
len(dfOdd[(df.isFraud == 1)]) / len(dfOdd)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.7046398891966759
```
:::

As you can see, the share of fraudulent transactions stands at 70%, so
this quirk seems to be a good feature at detecting fraud in
transactions. However, it is important to ask ourselves how this quirk
got into our data in the first place. One possibility could be that the
transactions never come through.

This could happen for a number of reasons including that there might be
another fraud prevention system in place that blocks the transactions,
or that the origin account for the transaction has insufficient funds.

While we have no way of verifying if there\'s another fraud prevention
system in place, we can check to see if the origin accounts have
insufficient funds. To do this, we have to run the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
len(dfOdd[(dfOdd.oldBalanceOrig <= dfOdd.amount)]) / len(dfOdd)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.8966412742382271
```
:::

As we can see in the output, close to 90% of the odd transactions have
insufficient funds in their origin accounts. From this, we can now
construct a rationale in which fraudsters try to drain a bank account of
all its funds more often than regular people do.

We need this rationale to avoid the data trap. Once established, the
rationale must be constantly scrutinized. In our case, it has failed to
explain 10% of the odd transactions, and if this number rises, it could
end up hurting the performance of our model in production.



[]{#ch02lvl1sec32}Preparing the data for the Keras library {#preparing-the-data-for-the-keras-library .title style="clear: both"}
----------------------------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

In [Chapter
1](https://subscription.packtpub.com/book/data/9781789136364/1){.link},
[*Neural Networks and Gradient-Based Optimization*]{.emphasis}, we saw
that neural networks would only take numbers as inputs. The[]{#id118
.indexterm} issue for us in our dataset is that not all of the
information in our table is numbers, some of it is presented as
characters.

Therefore, in this section, we\'re going to work on preparing the data
for Keras so that we can meaningfully work with it.

Before we start, let\'s[]{#id119 .indexterm} look at the three types of
data, [*Nominal*]{.emphasis}, [*Ordinal*]{.emphasis}, and
[*Numerical*]{.emphasis}:

::: {.itemizedlist}
-   [**Nominal data**]{.strong}: This comes in discrete categories that
    cannot be ordered. In our case, the type of[]{#id120 .indexterm}
    transfer is a nominal variable. There are four discrete types, but
    it does not make sense to put them in any order. For instance,
    TRANSFER cannot be more than CASH\_OUT, so instead, they are just
    separate categories.

-   [**Ordinal data**]{.strong}: This also[]{#id121 .indexterm} comes in
    discrete categories, but unlike nominal data, it can be ordered. For
    example, if coffee comes in large, medium, and small sizes, those
    are distinct categories because they can be compared. The large size
    contains more coffee than the small size.

-   [**Numerical data**]{.strong}: This []{#id122 .indexterm}can be
    ordered, but we can also perform mathematical operations on it. An
    example in our data is the number of funds, as we can both compare
    the amounts, and also subtract or add them up.
:::

Both nominal and ordinal []{#id123 .indexterm}data are [**categorical
data**]{.strong}, as they describe discrete categories. While numerical
data works fine with neural networks, only out of the box, categorical
data needs special treatment.

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec23}One-hot encoding {#one-hot-encoding .title}

</div>

</div>
:::

The most commonly []{#id124 .indexterm}used method to encode categorical
data is called [**one-hot encoding**]{.strong}. In one-hot []{#id125
.indexterm}encoding, we create a new variable, a so-called [**dummy
variable**]{.strong} for each category. We then set the dummy []{#id126
.indexterm}variable to 1 if the transaction is a member of a certain
category and to zero otherwise.

An example of how we could apply this to our data set can be seen as
follows:

So, this is what the categorical data would look like before one-hot
encoding:

::: {.informaltable}
  Transaction   Type
  ------------- -----------
  1             TRANSFER
  2             CASH\_OUT
  3             TRANSFER
:::

This is what the data would look like after one-hot encoding:

::: {.informaltable}
  Transaction   Type\_TRANSFER   Type\_CASH\_OUT
  ------------- ---------------- -----------------
  1             1                0
  2             0                1
  3             1                0
:::

The Pandas software[]{#id127 .indexterm} library offers a function that
allows us to create dummy variables out of the box. Before doing so,
however, it makes sense to add `Type_`{.literal} in front[]{#id128
.indexterm} of all actual transaction types. The dummy variables will be
named after the category. By adding `Type_`{.literal} to the beginning,
we know that these dummy variables indicate the type.

The following line of code does three things. Firstly,
`df['type'].astype(str)`{.literal} converts all the entries in the
[**Type**]{.strong} column to strings. Secondly, the `Type_`{.literal}
prefix is added as a result of combining the strings. Thirdly, the new
column of combined strings then replaces the original
[**Type**]{.strong} column:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
df['type'] = 'Type_' + df['type'].astype(str)
```
:::

We can now get the dummy variables by running the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
dummies = pd.get_dummies(df['type'])
```
:::

We should note that the `get_dummies()`{.literal} function creates a new
data frame. Next we attach this data frame to the main data frame, which
can be done by running:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
df = pd.concat([df,dummies],axis=1)
```
:::

The `concat()`{.literal} method, as seen in the preceding code,
concatenates two data frames. We concatenate along axis 1 to add the
data frame as new columns. Now that the dummy variables are in our main
data frame, we can remove the original column by running this:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
del df['type']
```
:::

And, voilà! We have turned our categorical variable into something a
neural network will be able to work with.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec24}Entity embeddings {#entity-embeddings .title}

</div>

</div>
:::

In this section, we\'re []{#id129 .indexterm}going to walk through
making use of both embeddings []{#id130 .indexterm}and the Keras
functional API, showing you the general workflow. Both of these topics
get introduced and explored fully in [Chapter
5](https://subscription.packtpub.com/book/data/9781789136364/5){.link},
[*Parsing Textual Data with Natural Language Processing*]{.emphasis},
where we will go beyond the general ideas presented here and where
we\'ll begin discussing topics like implementation.

It\'s fine if you do not understand everything that is going on just
now; this is an advanced section after all. If you want to use both of
these techniques, you will be well prepared after reading this book, as
we explain different elements of both methods throughout the book.

In this section, we will be creating embedding vectors for categorical
data. Before we start, we need to understand that embedding vectors are
vectors representing categorical values. We use embedding vectors as
inputs for neural networks. We train embeddings together with a neural
network, so that we can, over time, obtain more useful embeddings.
Embeddings are an extremely useful tool to have at our disposal.

Why are embeddings so useful? Not only do embeddings reduce the number
of dimensions needed []{#id131 .indexterm}for encoding over one-hot
encoding and thus decrease memory usage, but they also reduce sparsity
in input activations, which helps reduce overfitting, and they can
encode semantic meanings as vectors. The same advantages that made
embeddings useful for text, [Chapter
5](https://subscription.packtpub.com/book/data/9781789136364/5){.link},
[*Parsing Textual Data with Natural Language Processing*]{.emphasis},
also make them useful for categorical data.

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

#### []{#ch02lvl3sec07}Tokenizing categories {#tokenizing-categories .title}

</div>

</div>
:::

Just as with text, we have to[]{#id132 .indexterm} tokenize the inputs
before feeding them into the embeddings layer. To do this, we have to
create a mapping dictionary that maps categories to a token. We can
achieve this by running:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
map_dict = {}
for token, value in enumerate(df['type'].unique()):
    map_dict[value] = token
```
:::

This code loops over all the unique type categories while counting
upward. The first category gets token 0, the second 1, and so on. Our
`map_dict`{.literal} looks like this:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
{'CASH_IN': 4, 'CASH_OUT': 2, 'DEBIT': 3, 'PAYMENT': 0, 'TRANSFER': 1}
```
:::

We can now apply this mapping to our data frame:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
df["type"].replace(map_dict, inplace=True)
```
:::

As a result, all types will now be replaced by their tokens.

We have to deal with the non-categorical values in our data frame
separately. We can create a list of columns that are not the type and
not the target like this:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
other_cols = [c for c in df.columns if ((c != 'type') and (c != 'isFraud'))]
```
:::
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

#### []{#ch02lvl3sec08}Creating input models {#creating-input-models .title}

</div>

</div>
:::

The model we are[]{#id133 .indexterm} creating will have two inputs: one
for the types with an embedding layer, and one for all other,
non-categorical variables. To combine them with more ease at a later
point, we\'re going to keep track of their inputs and outputs with two
arrays:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
inputs = []
outputs = []
```
:::

The model that acts as an input for the type receives a one-dimensional
input and parses it through an embedding layer. The outputs of the
embedding layer are then reshaped into flat arrays, as we can see in
this code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
num_types = len(df['type'].unique()) 
type_embedding_dim = 3

type_in = Input(shape=(1,))
type_embedding = Embedding(num_types,type_embedding_dim,input_ length=1)(type_in)
type_out = Reshape(target_shape=(type_embedding_dim,))(type_embedding)

type_model = Model(type_in,type_out)

inputs.append(type_in)
outputs.append(type_out)
```
:::

The `type`{.literal} embeddings have three layers here. This is an
arbitrary choice, and experimentation with different numbers of
dimensions could improve the results.

For all the other inputs, we create another input that has as many
dimensions as there are non-categorical variables and consists of a
single dense layer with no activation function. The dense layer is
optional; the inputs could also be directly passed into the head model.
More layers could also be added, including these:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
num_rest = len(other_cols)

rest_in = Input(shape = (num_rest,)) 
rest_out = Dense(16)(rest_in)

rest_model = Model(rest_in,rest_out)

inputs.append(rest_in)
outputs.append(rest_out)
```
:::

Now that we have created the two input models, we can concatenate them.
On top of the two concatenated inputs, we will also build our head
model. To begin this process, we must first run the following:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
concatenated = Concatenate()(outputs)
```
:::

Then, by running[]{#id134 .indexterm} the following code, we can build
and compile the overall model:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
x = Dense(16)(concatenated)
x = Activation('sigmoid')(x)
x = Dense(1)(concatenated)
model_out = Activation('sigmoid')(x)

merged_model = Model(inputs, model_out)
merged_model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
```
:::
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

#### []{#ch02lvl3sec09}Training the model {#training-the-model .title}

</div>

</div>
:::

In this section we\'re going to[]{#id135 .indexterm} train a model with
multiple inputs. To do this, we need to provide a list of
[*X*]{.emphasis} values for each input. So, firstly we must split up our
data frame. We can do this by running the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
types = df['type']
rest = df[other_cols]
target = df['isFraud']
```
:::

Then, we can train the model by providing a list of the two inputs and
the target, as we can see in the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
history = merged_model.fit([types.values,rest.values],target.values, epochs = 1, batch_size = 128)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out:
Epoch 1/1
6362620/6362620 [==============================] - 78s 12us/step - loss: 0.0208 - acc: 0.9987
```


[]{#ch02lvl1sec33}Creating predictive models with Keras {#creating-predictive-models-with-keras .title style="clear: both"}
-------------------------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

Our[]{#id136 .indexterm} data now contains[]{#id137 .indexterm} the
following columns:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
amount, 
oldBalanceOrig, 
newBalanceOrig, 
oldBalanceDest, 
newBalanceDest, 
isFraud, 
isFlaggedFraud, 
type_CASH_OUT, 
type_TRANSFER, isNight
```
:::

Now that we\'ve got the columns, our data is prepared, and we can use it
to create a model.

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec25}Extracting the target {#extracting-the-target .title}

</div>

</div>
:::

To train the model, a[]{#id138 .indexterm} neural network needs a
target. In our case, `isFraud`{.literal} is the target, so we have to
separate it from the rest of the data. We can do this by running:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
y_df = df['isFraud']
x_df = df.drop('isFraud',axis=1)
```
:::

The first step only returns the `isFraud`{.literal} column and assigns
it to `y_df`{.literal}.

The second step returns all columns except `isFraud`{.literal} and
assigns them to `x_df`{.literal}.

We also need to convert our data from a pandas `DataFrame`{.literal} to
NumPy arrays. The pandas `DataFrame`{.literal} is built on top of NumPy
arrays but comes with lots of extra bells and whistles that make all the
preprocessing we did earlier possible. To train a neural network,
however, we just need the underlying data, which we can get by simply
running the following:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
y = y_df.values
X = x_df.values
```
:::
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec26}Creating a test set {#creating-a-test-set .title}

</div>

</div>
:::

When we train our model, we run the[]{#id139 .indexterm} risk of
[**overfitting**]{.strong}. Overfitting means that our model memorizes
the [*x*]{.emphasis} and [*y*]{.emphasis} mapping in our training
dataset but does not find the function that describes[]{#id140
.indexterm} the true relationship between [*x*]{.emphasis} and
[*y*]{.emphasis}. This is problematic because once we run our model
[**out of sample**]{.strong} -- that is, on data not in our training
set, it might do very poorly. To prevent this, we\'re going to create a
so-called [**test set**]{.strong}.

A test set is a holdout dataset, which[]{#id141 .indexterm} we only use
to evaluate our model once we think it is doing fairly well in order to
see how well it performs on data it has not seen yet. A test set is
usually randomly sampled from the complete data. Scikit-learn offers a
convenient function to do this, as we can see in the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
```
:::

The element, `train_test_split`{.literal} will randomly assign rows to
either the train or test set. You can specify `test_size`{.literal}, the
share of data that goes into the test set (which in our case is 33%), as
well as a random state. Assigning `random_state`{.literal} makes sure
that although the process is pseudo-random, it will always return the
same split, which makes our work more[]{#id142 .indexterm} reproducible.
Note that the actual choice of number (for example, `42`{.literal}) does
not really matter. What matters is that the same number is used in all
experiments.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec27}Creating a validation set {#creating-a-validation-set .title}

</div>

</div>
:::

Now you might be tempted to just try out a lot of different models until
you get a really high performance on the test set. However, ask yourself
this: how would you know that you have not selected a model that by
chance works well on the test set but does not work in real life?

The answer is that every time you evaluate on the test set, you incur a
bit of \"information leakage,\" that is, information from the test set
leaks into your model by influencing your choice of model. Gradually,
the test set becomes less valuable. The validation set is a sort of a
\"dirty test set\" that you can use to frequently test your models out
of sample performance without worrying. Though it\'s key to note that we
don\'t want to use the test set too often, but it is still used to
measure out-of-sample performance frequently.

To this end, we\'ll create a \"validation set,\" also known as a
development set.

We can do this the same way we created the test set, by just splitting
the training data again, as we can see in the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=42)
```
:::
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec28}Oversampling the training data {#oversampling-the-training-data .title}

</div>

</div>
:::

Remember that in our []{#id143 .indexterm}dataset, only a tiny fraction
of transactions were fraudulent, and that a model that is always
classifying transactions as genuine would have a very high level of
accuracy. To make sure we train our model on true relationships, we can
[**oversample**]{.strong} the training data.

This means that we would add data that would be fraudulent to our
dataset until we have the same amount of fraudulent transactions as
genuine transactions.

::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### []{#note07}Note {#note .title}

[**Note**]{.strong}: A useful library for this kind of task is
`imblearn`{.literal}, which includes a SMOTE function. See,
<http://contrib.scikitlearn.org/imbalanced-learn/>.
:::

[**Synthetic Minority Over-sampling Technique**]{.strong}
([**SMOTE**]{.strong}) is a clever way of oversampling. This
method[]{#id144 .indexterm} tries to create new samples while
maintaining the same decision boundaries for the classes. We can
oversample with SMOTE by simply running:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
From imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_sample(X_train, y_train)
```
:::
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec29}Building the model {#building-the-model .title}

</div>

</div>
:::

We\'ve successfully []{#id145 .indexterm}addressed several key learning
points, and so it\'s now finally time to build a neural network! As in
[Chapter
1](https://subscription.packtpub.com/book/data/9781789136364/1){.link},
[*Neural Networks and Gradient-Based Optimization*]{.emphasis}, we need
to import the required Keras modules using the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from keras.models import Sequential
from keras.layers import Dense, Activation
```
:::

In practice, many structured data problems require very low learning
rates. To set the learning rate for the gradient descent optimizer, we
also need to import the optimizer. We can do this by running:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from keras.optimizers import SGD
```
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

#### []{#ch02lvl3sec10}Creating a simple baseline {#creating-a-simple-baseline .title}

</div>

</div>
:::

Before we dive into more []{#id146 .indexterm}advanced models, it is
wise to start with a simple logistic regression baseline. This is to
make sure that our model can actually train successfully.

To create a simple baseline, we need to run the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
model = Sequential()
model.add(Dense(1, input_dim=9))
model.add(Activation('sigmoid'))
```
:::

You can see here a logistic regressor, which is the same as a one-layer
neural network:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-5), metrics=['acc'])
```
:::

Here, we will compile the model. Instead of just passing `SGD`{.literal}
to specify the optimizer for Stochastic Gradient Descent, we\'ll create
a custom instance of SGD in which we set the learning rate to 0.00001.
In this example, tracking accuracy is not needed since we evaluate our
models using the F1 score. Still, it still reveals some interesting
behavior, as you can see in the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
model.fit(X_train_res,y_train_res, epochs=5, batch_size=256, validation_data=(X_val,y_val))
```
:::

Notice how we have passed the validation data into Keras by creating a
tuple in which we store data and labels. We will train this model for 5
epochs:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
Train on 3331258 samples, validate on 185618 samples Epoch 1/5
3331258/3331258 [==============================] - 20s 6us/step - loss: 
3.3568 - acc: 0.7900 - val_loss: 3.4959 - val_acc: 0.7807 Epoch 2/5
3331258/3331258 [==============================] - 20s 6us/step - loss: 
3.0356 - acc: 0.8103 - val_loss: 2.9473 - val_acc: 0.8151 Epoch 3/5
3331258/3331258 [==============================] - 20s 6us/step - loss: 
2.4450 - acc: 0.8475 - val_loss: 0.9431 - val_acc: 0.9408 Epoch 4/5
3331258/3331258 [==============================] - 20s 6us/step - loss: 
2.3416 - acc: 0.8541 - val_loss: 1.0552 - val_acc: 0.9338 Epoch 5/5
3331258/3331258 [==============================] - 20s 6us/step - loss: 
2.3336 - acc: 0.8546 - val_loss: 0.8829 - val_acc: 0.9446
```
:::

Notice a few things here: first, we have trained on about 3.3 million
samples, which is more data than we initially had. The sudden increase
comes from the oversampling that we did earlier on in this[]{#id147
.indexterm} chapter. Secondly, the training set\'s accuracy
is significantly lower than the validation set\'s accuracy. This is
because the training set is balanced, while the validation set is not.

We oversampled the data by adding more fraud cases to the training set
than there are in real life, which as we discussed, helped our model
detect fraud better. If we did not oversample, our model would be
inclined to classify all transactions as genuine since the vast majority
of samples in the training set are genuine.

By adding fraud cases, we are forcing the model to learn what
distinguishes a fraud case. Yet, we want to validate our model on
realistic data. Therefore, our validation set does not artificially
contain many fraud cases.

A model classifying everything as genuine would have over 99% accuracy
on the validation set, but just 50% accuracy on the training set.
Accuracy is a flawed metric for such imbalanced datasets. It is a
half-decent proxy and more interpretable than just a loss, which is why
we keep track of it in Keras.

To evaluate our model, we should use the F1 score that we discussed at
the beginning of this chapter. However, Keras is unable to directly
track the F1 score in training since the calculation of an F1 score is
somewhat slow and would end up slowing down the training of our model.

::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### []{#note08}Note {#note-1 .title}

[**Note**]{.strong}: Remember that accuracy on an imbalanced dataset can
be very high, even if the model is performing poorly.
:::

If the model exhibits a higher degree of accuracy on an imbalanced
validation set than compared to that seen with a balanced training set,
then it says little about the model performing well.

Compare the training set\'s performance against the previous training
set\'s performance, and likewise the validation set\'s performance
against the previous validation set\'s performance. However, be careful
when comparing the training set\'s performance to that of the validation
set\'s[]{#id148 .indexterm} performance on highly imbalanced data.
However, if your data is equally balanced, then comparing the validation
set and the training set is a good way to gauge overfitting.

We are now in a position where we can make predictions on our test set
in order to evaluate the baseline. We start by using
`model.predict`{.literal} to make predictions on the test set:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
y_pred = model.predict(X_test)
```
:::

Before evaluating our baseline, we need to turn the probabilities given
by our model into absolute predictions. In our example, we\'ll classify
everything that has a fraud probability above 50% as fraud. To do this,
we need to run the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
y_pred[y_pred > 0.5] = 1
y_pred[y_pred < 0.5] = 0
```
:::

Our F1 score is already significantly better than it was for the
heuristic model, which if you go back, you\'ll see that it only achieved
a rate of 0.013131315551742895:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
f1_score(y_pred=y_pred,y_true=y_test)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.054384286716408395
```
:::

By plotting the confusion matrix, we\'re able to see that our
feature-based model has indeed improved on the heuristic model:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
cm = confusion_matrix(y_pred=y_pred,y_true=y_test)
plot_confusion_matrix(cm,['Genuine','Fraud'], normalize=False)
```
:::

This code should produce the following confusion matrix:

::: {.mediaobject}
![](8_files/B10354_02_05.jpg)

::: {.caption}
A confusion matrix for a simple Keras model
:::
:::

But what if we wanted to build more complex models that can express more
subtle relationships, than the one that we\'ve just built? Let\'s now do
that!
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

#### []{#ch02lvl3sec11}Building more complex models {#building-more-complex-models .title}

</div>

</div>
:::

After we have created a []{#id149 .indexterm}simple baseline, we can go
on to more complex models. The following code is an example of a
two-layer network:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
model = Sequential() 
model.add(Dense(16,input_dim=9)) 
model.add(Activation('tanh')) 
model.add(Dense(1)) 
model.add(Activation('sigmoid'))

model.compile(loss='binary_crossentropy',optimizer=SGD(lr=1e-5), metrics=['acc'])

model.fit(X_train_res,y_train_res, epochs=5, batch_size=256, validation_data=(X_val,y_val))

y_pred = model.predict(X_test)
y_pred[y_pred > 0.5] = 1
y_pred[y_pred < 0.5] = 0
```
:::

After running that code, we\'ll then again benchmark with the F1 score:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
f1_score(y_pred=y_pred,y_true=y_test)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.087220701988752675
```
:::

In this case, the more complex model does better than the simple
baseline created earlier. It seems as though the[]{#id150 .indexterm}
function mapping transaction data to fraud is complex and can be
approximated better by a deeper network.

In this section we have built and evaluated both simple and complex
neural network models for fraud detection. We have been careful to use
the validation set to gauge the initial out-of-sample performance.

With all of that, we can build much more complex neural networks (and we
will). But first we will have a look at the workhorse of modern
enterprise-ready machine learning: tree-based methods.



[]{#ch02lvl1sec34}A brief primer on tree-based methods {#a-brief-primer-on-tree-based-methods .title style="clear: both"}
------------------------------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

No chapter on structured data would be complete without mentioning
tree-based methods, such as random[]{#id151 .indexterm} forests or
XGBoost.

It is worth knowing about them because, in the realm of predictive
modeling for structured data, tree-based[]{#id152 .indexterm} methods
are very successful. However, they do not perform as well on more
advanced tasks, such as image recognition or sequence-to-sequence
modeling. This is the reason why the rest of the book does not deal with
tree-based methods.

::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### []{#note09}Note {#note .title}

[**Note**]{.strong}: For a[]{#id153 .indexterm} deeper dive into
XGBoost, check out the tutorials on the XGBoost documentation page:
[http://xgboost.readthedocs.io](http://xgboost.readthedocs.io/){.ulink}.
There is a nice explanation of how tree-based methods and gradient
boosting work in theory and practice under the [**Tutorials**]{.strong}
section of the website.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec30}A simple decision tree {#a-simple-decision-tree .title}

</div>

</div>
:::

The basic idea behind tree-based methods is the decision tree. A
decision tree splits up data to create the maximum[]{#id154 .indexterm}
difference in outcomes.

Let\'s assume for a second that our `isNight`{.literal} feature is the
greatest predictor of fraud. A decision tree would split our dataset
according to whether the transactions happened at night or not. It would
look at all the night-time transactions, looking for the next best
predictor of fraud, and it would do the same for all day-time
transactions.

Scikit-learn has a handy decision tree module. We can create one for our
data by simply running the following code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from sklearn.tree import DecisionTreeClassifier
dtree=DecisionTreeClassifier()
dtree.fit(X_train,y_train)
```
:::

The resulting tree will look like this:

::: {.mediaobject}
![](9_files/B10354_02_06.jpg)

::: {.caption}
A decision tree for fraud detection
:::
:::

Simple decision[]{#id155 .indexterm} trees, like the one we\'ve
produced, can give a lot of insight into data. For example, in our
decision tree, the most important feature seems to be the old balance of
the origin account, given that it is the first node in the tree.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec31}A random forest {#a-random-forest .title}

</div>

</div>
:::

A more advanced []{#id156 .indexterm}version of a simple decision tree
is a random forest, which is a collection of decision trees. A forest is
trained by taking subsets of the training data and training decision
trees on those subsets.

Often, those subsets do not include every feature of the training data.
By doing it this way, the different decision trees can fit different
aspects of the data and capture more information on aggregate. After a
number of trees have been created, their predictions are averaged to
create the final prediction.

The idea is that the errors presented by the trees are not correlated,
and so by using multiple trees you cancel out the error. You can create
and train a random forest classifier like this:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=10,n_jobs=-1)
rf.fit(X_train_res,y_train_res)
```
:::

You\'ll notice that with the code we\'ve just generated, random forests
have far fewer knobs to tune than neural networks. In this case, we just
specify the number of estimators, that is, the number of trees we would
like our forest to have.

The `n_jobs`{.literal} argument tells the random forest how many trees
we would like to train in parallel. Note that `-1`{.literal} stands for
\"as many as there are CPU cores\":

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
y_pred = rf.predict(X_test)
f1_score(y_pred=y_pred,y_true=y_test)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.8749502190362406
```
:::

The random forest does an order of magnitude better than the neural
network as its F1 score is close to 1, which is the maximum score. Its
confusion plot, seen []{#id157 .indexterm}as follows, shows that the
random forest significantly reduced the number of false positives:

::: {.mediaobject}
![](9_files/B10354_02_07.jpg)

::: {.caption}
A confusion matrix for the random forest
:::
:::

A shallow learning approach, such as a random forest, often does better
than deep learning on relatively simple problems. The reason for this is
that simple relationships with low-dimensional data can be hard to learn
for a deep learning model, which has to fit multiple parameters exactly
in order to match the simple function.

As we will see in later chapters of this book, as soon as relationships
do get more complex, deep learning gets to shine.
:::

::: {.section lang="en" lang="en"}
::: {.titlepage}
<div>

<div>

### []{#ch02lvl2sec32}XGBoost {#xgboost .title}

</div>

</div>
:::

[**XGBoost**]{.strong} stands for [**eXtreme Gradient
Boosting**]{.strong}. The idea behind gradient boosting is to train a
decision tree, and[]{#id158 .indexterm} then to train a second decision
tree on the errors that the first decision[]{#id159 .indexterm} tree
made.

Through this method, multiple layers of decision trees can be added,
which slowly reduces the total number of model errors. XGBoost is a
popular library that implements gradient boosting very efficiently.

::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### []{#note10}Note {#note-1 .title}

[**Note**]{.strong}: XGBoost is[]{#id160 .indexterm} installed on Kaggle
kernels by default. If you are running these examples locally, see the
XGBoost manual for installation instructions and more information:
<http://xgboost.readthedocs.io/>.
:::

Gradient boosting classifiers can be created and trained just like
random forests from `sklearn`{.literal}, as can be seen in the following
code:

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
import xgboost as xgb
booster = xgb.XGBClassifier(n_jobs=-1)
booster = booster.fit(X_train,y_train)
y_pred = booster.predict(X_test)
f1_score(y_pred=y_pred,y_true=y_test)
```
:::

::: {.informalexample}
::: {.toolbar .clearfix}
Copy
:::

``` {.programlisting .language-markup}
out: 0.85572959604286891
```
:::

The gradient booster[]{#id161 .indexterm} performs at almost the same
level as a random forest on this task. A common approach that is used is
to take both a random forest and a gradient booster and to average the
predictions in order to get an even better model.

The bulk of machine learning jobs in business today are done on
relatively simple structured data. The methods we have learned today,
random forests and gradient boosting, are therefore the standard tools
that most practitioners use in the real world.

In most enterprise machine learning applications, value creation does
not come from carefully tweaking a model or coming up with cool
architectures, but from massaging data and creating good features.
However, as tasks get more complex and more semantic understanding of
unstructured data is needed, these tools begin to fail.



[]{#ch02lvl1sec35}E2E modeling {#e2e-modeling .title style="clear: both"}
------------------------------

</div>

</div>

------------------------------------------------------------------------
:::

Our current approach[]{#id162 .indexterm} relies on engineered features.
As we discussed at the start of this chapter, an alternative method is
E2E modeling. In E2E modeling, both raw and unstructured data about a
transaction is used. This could include the description text of a
transfer, video feeds from cameras monitoring a cash machine, or other
sources of data. E2E is often more successful than feature engineering,
provided that you have enough data available.

To get valid results, and to successfully train the data with an E2E
model it can take millions of examples. Yet, often this is the only way
to gain an acceptable result, especially when it is[]{#id163 .indexterm}
hard to codify the rules for something. Humans can recognize things in
images well, but it is hard to come up with exact rules that distinguish
things, which is where E2E shines.

In the dataset used for this chapter, we do not have access to more
data, but the rest of the chapters of this book demonstrate various E2E
models.



[]{#ch02lvl1sec36}Exercises {#exercises .title style="clear: both"}
---------------------------

</div>

</div>

------------------------------------------------------------------------
:::

If you visit [https://kaggle.com](https://kaggle.com/){.ulink}, search
for a competition that has structured data. One example is the Titanic
competition. Here you can create a new kernel, do some feature
engineering, and try to build a predictive model.

How much can you improve it by investing time in feature engineering
versus model tweaking? Is there an E2E approach to the problem?


[]{#ch02lvl1sec37}Summary {#summary .title style="clear: both"}
-------------------------

</div>

</div>

------------------------------------------------------------------------
:::

In this chapter, we have taken a structured data problem from raw data
to strong and reliable predictive models. We have learned about
heuristic, feature engineering, and E2E modeling. We have also seen the
value of clear evaluation metrics and baselines.

In the next chapter, we will look into a field where deep learning truly
shines, computer vision. Here, we will discover the computer vision
pipeline, from working with simple models to very deep networks
augmented with powerful preprocessing software. The ability to \"see\"
empowers computers to enter completely new domains.
