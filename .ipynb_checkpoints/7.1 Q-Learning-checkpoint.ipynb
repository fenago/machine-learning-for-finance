{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we import some libraries\n",
    "#Json for loading and saving the model (optional)\n",
    "import json\n",
    "#matplotlib for rendering\n",
    "import matplotlib.pyplot as plt\n",
    "#numpy for handeling matrix operations\n",
    "import numpy as np\n",
    "#time, to, well... keep track of time\n",
    "import time\n",
    "#Python image libarary for rendering\n",
    "from PIL import Image\n",
    "#iPython display for making sure we can render the frames\n",
    "from IPython import display\n",
    "#seaborn for rendering\n",
    "import seaborn\n",
    "#Keras is a deep learning libarary\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "\n",
    "#Setup matplotlib so that it runs nicely in iPython\n",
    "%matplotlib inline\n",
    "#setting up seaborn\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the game\n",
    "\n",
    "This is the code for the actual game we are training on.\n",
    "Catch is a simple game you might have played as a child. In the game, fruits, represented by white tiles, fall from the top. The goal is to catch the fruits with a basked (represented by white tiles, this is deep learning, not game design). If you catch a fruit, you get a point (your score goes up by one), if you miss a fruit, you loose one (your score goes down).\n",
    "\n",
    "Don't worry all too much about the details of the implementation, the focus here should be on the AI, not on the game.\n",
    "Just make sure you run this cell so that it is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catch(object):\n",
    "    \"\"\"\n",
    "    Class catch is the actual game.\n",
    "    In the game, fruits, represented by white tiles, fall from the top.\n",
    "    The goal is to catch the fruits with a basked (represented by white tiles, this is deep learning, not game design).\n",
    "    \"\"\"\n",
    "    def __init__(self, grid_size=10):\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        \"\"\"\n",
    "        Input: action and states\n",
    "        Ouput: new states and reward\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        if action == 0:  # left\n",
    "            action = -1\n",
    "        elif action == 1:  # stay\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1  # right\n",
    "        f0, f1, basket = state[0]\n",
    "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
    "        f0 += 1\n",
    "        out = np.asarray([f0, f1, new_basket])\n",
    "        out = out[np.newaxis]\n",
    "\n",
    "        assert len(out.shape) == 2\n",
    "        self.state = out\n",
    "\n",
    "    def _draw_state(self):\n",
    "        im_size = (self.grid_size,)*2\n",
    "        state = self.state[0]\n",
    "        canvas = np.zeros(im_size)\n",
    "        canvas[state[0], state[1]] = 1  # draw fruit\n",
    "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
    "        return canvas\n",
    "        \n",
    "    def _get_reward(self):\n",
    "        fruit_row, fruit_col, basket = self.state[0]\n",
    "        if fruit_row == self.grid_size-1:\n",
    "            if abs(fruit_col - basket) <= 1:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _is_over(self):\n",
    "        if self.state[0, 0] == self.grid_size-1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self._draw_state()\n",
    "        return canvas.reshape((1, -1))\n",
    "\n",
    "    def act(self, action):\n",
    "        self._update_state(action)\n",
    "        reward = self._get_reward()\n",
    "        game_over = self._is_over()\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
    "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
    "        self.state = np.asarray([0, n, m])[np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to defining the game we need to define some helper variables and functions.\n",
    "Run the cells below to define them, then we will get to the meat and the potatoes of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define some variables used for the game and rendering later\n",
    "\"\"\"\n",
    "#last frame time keeps track of which frame we are at\n",
    "last_frame_time = 0\n",
    "#translate the actions to human readable words\n",
    "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
    "#size of the game field\n",
    "grid_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(action,points,input_t):\n",
    "    #Function used to render the game screen\n",
    "    #Get the last rendered frame\n",
    "    global last_frame_time\n",
    "    print(\"Action %s, Points: %d\" % (translate_action[action],points))\n",
    "    #Only display the game screen if the game is not over\n",
    "    if(\"End\" not in translate_action[action]):\n",
    "        #Render the game with matplotlib\n",
    "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
    "               interpolation='none', cmap='gray')\n",
    "        #Clear whatever we rendered before\n",
    "        display.clear_output(wait=True)\n",
    "        #And display the rendering\n",
    "        display.display(plt.gcf())\n",
    "    #Update the last frame time\n",
    "    last_frame_time = set_max_fps(last_frame_time)\n",
    "    \n",
    "    \n",
    "def set_max_fps(last_frame_time,FPS = 1):\n",
    "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
    "    if sleep_time > 0:\n",
    "        time.sleep(sleep_time)\n",
    "    return current_milli_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    \"\"\"\n",
    "    During gameplay all the experiences < s, a, r, s’ > are stored in a replay memory. \n",
    "    In training, batches of randomly drawn experiences are used to generate the input and target for training.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        \"\"\"\n",
    "        Setup\n",
    "        max_memory: the maximum number of experiences we want to store\n",
    "        memory: a list of experiences\n",
    "        discount: the discount factor for future experience\n",
    "        \n",
    "        In the memory the information whether the game ended at the state is stored seperately in a nested array\n",
    "        [...\n",
    "        [experience, game_over]\n",
    "        [experience, game_over]\n",
    "        ...]\n",
    "        \"\"\"\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        #Save a state to memory\n",
    "        self.memory.append([states, game_over])\n",
    "        #We don't want to store infinite memories, so if we have too many, we just delete the oldest one\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        \n",
    "        #How many experiences do we have?\n",
    "        len_memory = len(self.memory)\n",
    "        \n",
    "        #Calculate the number of actions that can possibly be taken in the game\n",
    "        num_actions = model.output_shape[-1]\n",
    "        \n",
    "        #Dimensions of the game field\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        \n",
    "        #We want to return an input and target vector with inputs from an observed state...\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        \n",
    "        #...and the target r + gamma * max Q(s’,a’)\n",
    "        #Note that our target is a matrix, with possible fields not only for the action taken but also\n",
    "        #for the other possible actions. The actions not take the same value as the prediction to not affect them\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        #We draw states to learn from randomly\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
    "                                                  size=inputs.shape[0])):\n",
    "            \"\"\"\n",
    "            Here we load one transition <s, a, r, s’> from memory\n",
    "            state_t: initial state s\n",
    "            action_t: action taken a\n",
    "            reward_t: reward earned r\n",
    "            state_tp1: the state that followed s’\n",
    "            \"\"\"\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            \n",
    "            #We also need to know whether the game ended at this state\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            #add the state s to the input\n",
    "            inputs[i:i+1] = state_t\n",
    "            \n",
    "            # First we fill the target values with the predictions of the model.\n",
    "            # They will not be affected by training (since the training loss for them is 0)\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            \n",
    "            \"\"\"\n",
    "            If the game ended, the expected reward Q(s,a) should be the final reward r.\n",
    "            Otherwise the target value is r + gamma * max Q(s’,a’)\n",
    "            \"\"\"\n",
    "            #  Here Q_sa is max_a'Q(s', a')\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "            \n",
    "            #if the game ended, the reward is the final reward\n",
    "            if game_over:  # if game_over is True\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                # r + gamma * max Q(s’,a’)\n",
    "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
    "        return inputs, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(grid_size,num_actions,hidden_size):\n",
    "    #seting up the model with keras\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(sgd(lr=.1), \"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "epsilon = .1  # exploration\n",
    "num_actions = 3  # [move_left, stay, move_right]\n",
    "max_memory = 500 # Maximum number of experiences we are storing\n",
    "hidden_size = 100 # Size of the hidden layers\n",
    "batch_size = 1 # Number of experiences we use for training per batch\n",
    "grid_size = 10 # Size of the playing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "model = baseline_model(grid_size,num_actions,hidden_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define environment/game\n",
    "env = Catch(grid_size)\n",
    "\n",
    "# Initialize experience replay object\n",
    "exp_replay = ExperienceReplay(max_memory=max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epochs, verbose = 1):\n",
    "    # Train\n",
    "    #Reseting the win counter\n",
    "    win_cnt = 0\n",
    "    # We want to keep track of the progress of the AI over time, so we save its win count history\n",
    "    win_hist = []\n",
    "    #Epochs is the number of games we play\n",
    "    for e in range(epochs):\n",
    "        loss = 0.\n",
    "        #Resetting the game\n",
    "        env.reset()\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "        \n",
    "        while not game_over:\n",
    "            #The learner is acting on the last observed game screen\n",
    "            #input_t is a vector containing representing the game screen\n",
    "            input_tm1 = input_t\n",
    "            \n",
    "            \"\"\"\n",
    "            We want to avoid that the learner settles on a local minimum.\n",
    "            Imagine you are eating eating in an exotic restaurant. After some experimentation you find \n",
    "            that Penang Curry with fried Tempeh tastes well. From this day on, you are settled, and the only Asian \n",
    "            food you are eating is Penang Curry. How can your friends convince you that there is better Asian food?\n",
    "            It's simple: Sometimes, they just don't let you choose but order something random from the menu.\n",
    "            Maybe you'll like it.\n",
    "            The chance that your friends order for you is epsilon\n",
    "            \"\"\"\n",
    "            if np.random.rand() <= epsilon:\n",
    "                #Eat something random from the menu\n",
    "                action = np.random.randint(0, num_actions, size=1)\n",
    "            else:\n",
    "                #Choose yourself\n",
    "                #q contains the expected rewards for the actions\n",
    "                q = model.predict(input_tm1)\n",
    "                #We pick the action with the highest expected reward\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            #If we managed to catch the fruit we add 1 to our win counter\n",
    "            if reward == 1:\n",
    "                win_cnt += 1        \n",
    "            \n",
    "            #Uncomment this to render the game here\n",
    "            #display_screen(action,3000,inputs[0])\n",
    "            \n",
    "            \"\"\"\n",
    "            The experiences < s, a, r, s’ > we make during gameplay are our training data.\n",
    "            Here we first save the last experience, and then load a batch of experiences to train our model\n",
    "            \"\"\"\n",
    "            \n",
    "            # store experience\n",
    "            exp_replay.remember([input_tm1, action, reward, input_t], game_over)    \n",
    "            \n",
    "            # Load batch of experiences\n",
    "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
    "  \n",
    "            # train model on experiences\n",
    "            batch_loss = model.train_on_batch(inputs, targets)\n",
    "            \n",
    "            #print(loss)\n",
    "            loss += batch_loss\n",
    "        if verbose > 0:\n",
    "            print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win count {}\".format(e,epochs, loss, win_cnt))\n",
    "        win_hist.append(win_cnt)\n",
    "    return win_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing many games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5000 # Number of games played in training, I found the model needs about 4,000 games till it plays well\n",
    "# Train the model\n",
    "# For simplicity of the noteb\n",
    "hist = train(model,epoch,verbose=0)\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    #This function lets a pretrained model play the game to evaluate how well it is doing\n",
    "    global last_frame_time\n",
    "    plt.ion()\n",
    "    # Define environment, game\n",
    "    env = Catch(grid_size)\n",
    "    #c is a simple counter variable keeping track of how much we train\n",
    "    c = 0\n",
    "    #Reset the last frame time (we are starting from 0)\n",
    "    last_frame_time = 0\n",
    "    #Reset score\n",
    "    points = 0\n",
    "    #For training we are playing the game 10 times\n",
    "    for e in range(10):\n",
    "        loss = 0.\n",
    "        #Reset the game\n",
    "        env.reset()\n",
    "        #The game is not over\n",
    "        game_over = False\n",
    "        # get initial input\n",
    "        input_t = env.observe()\n",
    "        #display_screen(3,points,input_t)\n",
    "        c += 1\n",
    "        while not game_over:\n",
    "            #The learner is acting on the last observed game screen\n",
    "            #input_t is a vector containing representing the game screen\n",
    "            input_tm1 = input_t\n",
    "            #Feed the learner the current status and get the expected rewards for different actions from it\n",
    "            q = model.predict(input_tm1)\n",
    "            #Select the action with the highest expected reward\n",
    "            action = np.argmax(q[0])\n",
    "            # apply action, get rewards and new state\n",
    "            input_t, reward, game_over = env.act(action)\n",
    "            #Update our score\n",
    "            points += reward\n",
    "            display_screen(action,points,input_t)\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_diff(a, n=100):\n",
    "    diff = np.diff(a)\n",
    "    ret = np.cumsum(diff, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(moving_average_diff(hist),antialiased=False)\n",
    "plt.ylabel('Average of victories per game')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "£"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
