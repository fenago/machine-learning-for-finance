{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4590,
     "status": "ok",
     "timestamp": 1526810261274,
     "user": {
      "displayName": "Jannes Klaas",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115843019312458301179"
     },
     "user_tz": -120
    },
    "id": "WvnZWyvt-ZPa",
    "outputId": "395cc121-bac6-4222-b727-12c46abcca59"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "EPISODES = 3000\n",
    "\n",
    "\n",
    "# A2C(Advantage Actor-Critic) agent for the Cartpole\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "        \n",
    "        self.exp_replay = deque(maxlen=2000)\n",
    "\n",
    "        # get gym environment name\n",
    "        # these are hyper parameters for the A3C\n",
    "        self.actor_lr = 0.0001\n",
    "        self.critic_lr = 0.001\n",
    "        self.discount_factor = .9\n",
    "\n",
    "        # create model for actor and critic network\n",
    "        self.actor, self.critic = self.build_model()\n",
    "\n",
    "        # method for training actor and critic network\n",
    "        #self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]\n",
    "        \n",
    "        self.optimize_actor = self.actor_optimizer() #5\n",
    "        self.optimize_critic = self.critic_optimizer() \n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        state = Input(batch_shape=(None, self.state_size))\n",
    "        actor_input = Dense(30, activation='relu', kernel_initializer='he_uniform')(state)\n",
    "        # actor_hidden = Dense(self.hidden2, activation='relu')(actor_input)\n",
    "        mu_0 = Dense(self.action_size, activation='tanh', kernel_initializer='he_uniform')(actor_input)\n",
    "        sigma_0 = Dense(self.action_size, activation='softplus', kernel_initializer='he_uniform')(actor_input)\n",
    "\n",
    "        mu = Lambda(lambda x: x * 2)(mu_0)\n",
    "        sigma = Lambda(lambda x: x + 0.0001)(sigma_0)\n",
    "\n",
    "        critic_input = Dense(30, activation='relu', kernel_initializer='he_uniform')(state)\n",
    "        # value_hidden = Dense(self.hidden2, activation='relu')(critic_input)\n",
    "        state_value = Dense(1, activation='linear', kernel_initializer='he_uniform')(critic_input)\n",
    "\n",
    "        actor = Model(inputs=state, outputs=(mu, sigma))\n",
    "        critic = Model(inputs=state, outputs=state_value)\n",
    "\n",
    "        actor._make_predict_function()\n",
    "        critic._make_predict_function()\n",
    "\n",
    "        actor.summary()\n",
    "        critic.summary()\n",
    "\n",
    "        return actor, critic\n",
    "\n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape=(None, 1))\n",
    "        advantages = K.placeholder(shape=(None, 1))\n",
    "\n",
    "        # mu = K.placeholder(shape=(None, self.action_size))\n",
    "        # sigma_sq = K.placeholder(shape=(None, self.action_size))\n",
    "\n",
    "        mu, sigma_sq = self.actor.output\n",
    "\n",
    "        pdf = 1. / K.sqrt(2. * np.pi * sigma_sq) * K.exp(-K.square(action - mu) / (2. * sigma_sq))\n",
    "        log_pdf = K.log(pdf + K.epsilon())\n",
    "        entropy = K.sum(0.5 * (K.log(2. * np.pi * sigma_sq) + 1.))\n",
    "\n",
    "        exp_v = log_pdf * advantages\n",
    "\n",
    "        exp_v = K.sum(exp_v + 0.01 * entropy)\n",
    "        actor_loss = -exp_v\n",
    "\n",
    "        optimizer = Adam(lr=self.actor_lr)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], actor_loss)\n",
    "\n",
    "        train = K.function([self.actor.input, action, advantages], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    # make loss function for Value approximation\n",
    "    def critic_optimizer(self):\n",
    "        discounted_reward = K.placeholder(shape=(None, 1))\n",
    "\n",
    "        value = self.critic.output\n",
    "\n",
    "        loss = K.mean(K.square(discounted_reward - value))\n",
    "\n",
    "        optimizer = Adam(lr=self.critic_lr)\n",
    "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
    "        train = K.function([self.critic.input, discounted_reward], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, state):\n",
    "        mu, sigma_sq = self.actor.predict(np.reshape(state, [1, self.state_size]))\n",
    "        # sigma_sq = np.log(np.exp(sigma_sq + 1))\n",
    "        epsilon = np.random.randn(self.action_size)\n",
    "        # action = norm.rvs(loc=mu, scale=sigma_sq,size=1)\n",
    "        action = mu + np.sqrt(sigma_sq) * epsilon\n",
    "        action = np.clip(action, -2, 2)\n",
    "        return action\n",
    "\n",
    "    # update policy network every episode\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        self.exp_replay.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        (state, action, reward, next_state, done) = random.sample(self.exp_replay,1)[0]\n",
    "      \n",
    "        target = np.zeros((1, self.value_size))\n",
    "        advantages = np.zeros((1, self.action_size))\n",
    "\n",
    "        value = self.critic.predict(state)[0]\n",
    "        next_value = self.critic.predict(next_state)[0]\n",
    "\n",
    "        if done:\n",
    "            advantages[0] = reward - value\n",
    "            target[0][0] = reward\n",
    "        else:\n",
    "            advantages[0] = reward + self.discount_factor * (next_value) - value\n",
    "            target[0][0] = reward + self.discount_factor * next_value\n",
    "\n",
    "        self.optimize_actor([state, action, advantages])\n",
    "        self.optimize_critic([state, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dg0doG69-4ix"
   },
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "  # In case of CartPole-v1, maximum length of episode is 500\n",
    "  env = gym.make('Pendulum-v0')\n",
    "  # get size of state and action from environment\n",
    "  state_size = env.observation_space.shape[0]\n",
    "  action_size = env.action_space.shape[0]\n",
    "\n",
    "  # make A2C agent\n",
    "  agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "  scores, episodes = [], []\n",
    "\n",
    "  for e in range(EPISODES):\n",
    "      done = False\n",
    "      score = 0\n",
    "      state = env.reset()\n",
    "      state = np.reshape(state, [1, state_size])\n",
    "\n",
    "      while not done:\n",
    "          if agent.render:\n",
    "              env.render()\n",
    "\n",
    "          action = agent.get_action(state)\n",
    "          next_state, reward, done, info = env.step(action)\n",
    "          reward /= 10\n",
    "          next_state = np.reshape(next_state, [1, state_size])\n",
    "          # if an action make the episode end, then gives penalty of -100\n",
    "          agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "          score += reward\n",
    "          state = next_state\n",
    "\n",
    "          if done:\n",
    "              # every episode, plot the play time\n",
    "              scores.append(score)\n",
    "              episodes.append(e)\n",
    "              print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "              # if the mean of scores of last 10 episode is bigger than 490\n",
    "              # stop training\n",
    "              if np.mean(scores[-min(10, len(scores)):]) > -20:\n",
    "                  print('Solved Pendulum-v0 after {} iterations'.format(len(scores)))\n",
    "                  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 39026
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3726464,
     "status": "error",
     "timestamp": 1526814016426,
     "user": {
      "displayName": "Jannes Klaas",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115843019312458301179"
     },
     "user_tz": -120
    },
    "id": "3XlAsQku-_hX",
    "outputId": "004a1239-d6bd-4651-83ac-4769b034805d"
   },
   "outputs": [],
   "source": [
    "scrs = []\n",
    "for i in range(10):\n",
    "  scrs.append(run_experiment())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "olYdDyoh_Cbt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "A2c w exp replay.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
